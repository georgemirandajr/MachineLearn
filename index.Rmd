---
title: "Machine Learning"
author: "Antonio Ferreras"
date: "Thursday, August 21, 2014"
output: html_document
---

### 1. Executive Summary

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

### Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely **quantify how well they do it**. The goal of this will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information available in [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

The goal of this project is to predict the manner in which they did the exercise. This is the *classe* variable in the training set. 

### 2. Data inspection

```{r}
train <- read.csv(".//pml-training.csv")
test <- read.csv(".//pml-testing.csv")
dim(train)
```

19622 records in the data set and 160 variables. The data is quite noisy, as we know for the paper, so the appropiate model could be a Random Forest, but there are too much data for fitting a model. Then the strategy for fitting a correct model is first reduce as much as possible the number of the variables, and then fit the Random Forest model and finally fit the parameters.

```{r}
library(caret)
library(rpart)
library(randomForest)
set.seed(1966)
```


### 3. Variable reduction

Several variables do not have to do with the measurements. Following the paper cited above we will not use them in the prediction model. Lets eliminate them.

* *problem_id. It is merely one enumeration of the measurements. 

```{r, echo=FALSE}
test$problem_id <- NULL
test$classe<- c(rep("A",10), rep("B",10))
combi <- rbind(test,train)
nzv <- nearZeroVar(test)
combi <- combi[,-nzv]
```

```{r cache=TRUE}
combi$X <- NULL
combi$raw_timestamp_part_1 <- NULL
combi$raw_timestamp_part_2 <- NULL
combi$cvtd_timestamp <- NULL
combi$num_window <- NULL
combi$user_name <- NULL
combi$classe <- factor(combi$classe)
ncol(combi)
```


```{r cache=TRUE}
modfit1 <- rpart(classe ~ ., data = combi[c(21:19642),], method="class")
pred_train_1 <- predict(modfit1, newdata=combi[c(21:19642),], type ="class")
confusionMatrix(pred_train_1, combi[c(21:19642),"classe"])$overall[1]
```

```{r cache=TRUE}
vars <- varImp(modfit1)
sum(vars > 0)
combi <- combi[, c(rownames(vars)[vars$Overall>0],"classe")]
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

### 4. Model Tunning

```{r cache=TRUE}
modfit3 <- randomForest(classe ~ ., data = combi[c(21:19642),], importance=TRUE)
pred_train_3 <- predict(modfit3, newdata=combi[c(21:19642),], type ="class")
confusionMatrix(pred_train_3, combi[c(21:19642),"classe"])$overall[1]

vars <- importance(modfit3)
nrow(vars)
summary(vars[,"MeanDecreaseGini"])
```


```{r cache=TRUE}
Giny <- vars[order(vars[,"MeanDecreaseGini"], decreasing = TRUE) ,"MeanDecreaseGini"]

for (i in c(1:11)) {
    Giny_o <- names(Giny[1:i+1])
    data1 <- combi[,c(Giny_o, "classe")]
    modfit4 <- randomForest(classe ~ ., data = data1[c(21:19642),], importance=TRUE)
    pred_train_4 <- predict(modfit4, newdata=data1[c(21:19642),], type ="class")
    y[i] <- confusionMatrix(pred_train_4, data1[c(21:19642),"classe"])$overall[1]
}
plot(c(2:12),y,type="b", xlab="Number of Variables", ylab="Accuracy", main="Random Forest", las=1, col="blue")
```


```{r cache=TRUE}
data1 = combi[,c(names(Giny[1:6]), "classe")]
modfit5 <- randomForest(classe ~ ., data = data1[c(21:19642),], importance=TRUE)
pred_test_5 <- predict(modfit5, newdata=data1[c(1:20),], type ="class")
pred_test_5
```


### 5. Conclusion

```{r cache=TRUE}
names(importance(modfit5)[,"MeanDecreaseGini"])
```

## Appendix

### A1. Final model

```{r}
pred_train_5 <- predict(modfit5, newdata=data1[c(21:19642),], type ="class")
confusionMatrix(pred_train_5, data1[c(21:19642),"classe"])
```





